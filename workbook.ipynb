{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def avg_medal_count():\n",
    "\n",
    "countries = ['Russian Fed.', 'Norway', 'Canada', 'United States',\n",
    "                 'Netherlands', 'Germany', 'Switzerland', 'Belarus',\n",
    "                 'Austria', 'France', 'Poland', 'China', 'Korea', \n",
    "                 'Sweden', 'Czech Republic', 'Slovenia', 'Japan',\n",
    "                 'Finland', 'Great Britain', 'Ukraine', 'Slovakia',\n",
    "                 'Italy', 'Latvia', 'Australia', 'Croatia', 'Kazakhstan','JhumriTalai']\n",
    "\n",
    "gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,0]\n",
    "silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0,0]\n",
    "bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1,0]\n",
    "    \n",
    "olympic_medal_counts = {'country_name':countries,\n",
    "                            'gold': Series(gold),\n",
    "                            'silver': Series(silver),\n",
    "                            'bronze': Series(bronze)}    \n",
    "df = DataFrame(olympic_medal_counts)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "#return avg_medal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #atleast_one_medal = \n",
    "newdf = df[(df['gold'] >=1) | (df['silver'] >=1) | df['bronze']>=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "avg_medal_count = newdf.mean()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_medal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['gold','silver','bronze']].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "olympic_medal_counts = {'country_name':countries,\n",
    "                            'gold': Series(gold),\n",
    "                            'silver': Series(silver),\n",
    "                            'bronze': Series(bronze)}    \n",
    "olympic_medal_counts_df = DataFrame(olympic_medal_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medals = olympic_medal_counts_df[['gold','silver','bronze']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.dot(medals,[4,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "olympic_points = {'country_name':Series(countries), 'points':Series(points)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "olympic_points_df = DataFrame(olympic_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning LabelEncoding and One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX = pd.read_csv('titanic_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = XX.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "List = ['Name','Sex','Ticket','Cabin','Embarked']\n",
    "for i in List:\n",
    "    print (X[i].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in X:\n",
    "    #print(X[feat].head())\n",
    "    X[i] = le.fit_transform(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohe.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehotlabels = ohe.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(onehotlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = XX\n",
    "#X2 = X2.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = X2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3 = ohe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful reference: http://napitupulu-jon.appspot.com/posts/datasets-questions.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data = pickle.load(open(\"./enron/ud120-projects-master/final_project/final_project_dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of items in the dictionary\n",
    "len(enron_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of features in the dict\n",
    "\n",
    "unique_features = set(\n",
    "    feature\n",
    "    for row_dict in enron_data.values()\n",
    "    for feature in row_dict.keys()\n",
    ")\n",
    "print(unique_features)\n",
    "# {'golf', 'delta', 'foxtrot', 'alpha', 'bravo', 'echo', 'tango', 'kilo'}\n",
    "print(len(unique_features))\n",
    "# 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listing out individual elements from the dictionary\n",
    "list(enron_data.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read poi names\n",
    "poi_names = pd.read_csv(\"./enron/ud120-projects-master/final_project/poi_names.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = enron_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(enron_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['SKILLING JEFFREY K']['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many POIs in the dataset?\n",
    "count = 0\n",
    "for user in enron_data:\n",
    "    if enron_data[user]['poi'] == True:\n",
    "        count+=1\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ./enron/ud120-projects-master/final_project/poi_email_addresses.py\n",
    "def poiEmails():\n",
    "    email_list = [\"kenneth_lay@enron.net\",    \n",
    "            \"kenneth_lay@enron.com\",\n",
    "            \"klay.enron@enron.com\",\n",
    "            \"kenneth.lay@enron.com\", \n",
    "            \"klay@enron.com\",\n",
    "            \"layk@enron.com\",\n",
    "            \"chairman.ken@enron.com\",\n",
    "            \"jeffreyskilling@yahoo.com\",\n",
    "            \"jeff_skilling@enron.com\",\n",
    "            \"jskilling@enron.com\",\n",
    "            \"effrey.skilling@enron.com\",\n",
    "            \"skilling@enron.com\",\n",
    "            \"jeffrey.k.skilling@enron.com\",\n",
    "            \"jeff.skilling@enron.com\",\n",
    "            \"kevin_a_howard.enronxgate.enron@enron.net\",\n",
    "            \"kevin.howard@enron.com\",\n",
    "            \"kevin.howard@enron.net\",\n",
    "            \"kevin.howard@gcm.com\",\n",
    "            \"michael.krautz@enron.com\"\n",
    "            \"scott.yeager@enron.com\",\n",
    "            \"syeager@fyi-net.com\",\n",
    "            \"scott_yeager@enron.net\",\n",
    "            \"syeager@flash.net\",\n",
    "            \"joe'.'hirko@enron.com\", \n",
    "            \"joe.hirko@enron.com\", \n",
    "            \"rex.shelby@enron.com\", \n",
    "            \"rex.shelby@enron.nt\", \n",
    "            \"rex_shelby@enron.net\",\n",
    "            \"jbrown@enron.com\",\n",
    "            \"james.brown@enron.com\", \n",
    "            \"rick.causey@enron.com\", \n",
    "            \"richard.causey@enron.com\", \n",
    "            \"rcausey@enron.com\",\n",
    "            \"calger@enron.com\",\n",
    "            \"chris.calger@enron.com\", \n",
    "            \"christopher.calger@enron.com\", \n",
    "            \"ccalger@enron.com\",\n",
    "            \"tim_despain.enronxgate.enron@enron.net\", \n",
    "            \"tim.despain@enron.com\",\n",
    "            \"kevin_hannon@enron.com\", \n",
    "            \"kevin'.'hannon@enron.com\", \n",
    "            \"kevin_hannon@enron.net\", \n",
    "            \"kevin.hannon@enron.com\",\n",
    "            \"mkoenig@enron.com\", \n",
    "            \"mark.koenig@enron.com\",\n",
    "            \"m..forney@enron.com\",\n",
    "            \"ken'.'rice@enron.com\", \n",
    "            \"ken.rice@enron.com\",\n",
    "            \"ken_rice@enron.com\", \n",
    "            \"ken_rice@enron.net\",\n",
    "            \"paula.rieker@enron.com\",\n",
    "            \"prieker@enron.com\", \n",
    "            \"andrew.fastow@enron.com\", \n",
    "            \"lfastow@pdq.net\", \n",
    "            \"andrew.s.fastow@enron.com\", \n",
    "            \"lfastow@pop.pdq.net\", \n",
    "            \"andy.fastow@enron.com\",\n",
    "            \"david.w.delainey@enron.com\", \n",
    "            \"delainey.dave@enron.com\", \n",
    "            \"'delainey@enron.com\", \n",
    "            \"david.delainey@enron.com\", \n",
    "            \"'david.delainey'@enron.com\", \n",
    "            \"dave.delainey@enron.com\", \n",
    "            \"delainey'.'david@enron.com\",\n",
    "            \"ben.glisan@enron.com\", \n",
    "            \"bglisan@enron.com\", \n",
    "            \"ben_f_glisan@enron.com\", \n",
    "            \"ben'.'glisan@enron.com\",\n",
    "            \"jeff.richter@enron.com\", \n",
    "            \"jrichter@nwlink.com\",\n",
    "            \"lawrencelawyer@aol.com\", \n",
    "            \"lawyer'.'larry@enron.com\", \n",
    "            \"larry_lawyer@enron.com\", \n",
    "            \"llawyer@enron.com\", \n",
    "            \"larry.lawyer@enron.com\", \n",
    "            \"lawrence.lawyer@enron.com\",\n",
    "            \"tbelden@enron.com\", \n",
    "            \"tim.belden@enron.com\", \n",
    "            \"tim_belden@pgn.com\", \n",
    "            \"tbelden@ect.enron.com\",\n",
    "            \"michael.kopper@enron.com\",\n",
    "            \"dave.duncan@enron.com\", \n",
    "            \"dave.duncan@cipco.org\", \n",
    "            \"duncan.dave@enron.com\",\n",
    "            \"ray.bowen@enron.com\", \n",
    "            \"raymond.bowen@enron.com\", \n",
    "            \"'bowen@enron.com\",\n",
    "            \"wes.colwell@enron.com\",\n",
    "            \"dan.boyle@enron.com\",\n",
    "            \"cloehr@enron.com\", \n",
    "            \"chris.loehr@enron.com\"\n",
    "        ]\n",
    "    return email_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(poiEmails())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ./enron/ud120-projects-master/final_project/poi_names.txt\n",
    "http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\n",
    "\n",
    "(y) Lay, Kenneth\n",
    "(y) Skilling, Jeffrey\n",
    "(n) Howard, Kevin\n",
    "(n) Krautz, Michael\n",
    "(n) Yeager, Scott\n",
    "(n) Hirko, Joseph\n",
    "(n) Shelby, Rex\n",
    "(n) Bermingham, David\n",
    "(n) Darby, Giles\n",
    "(n) Mulgrew, Gary\n",
    "(n) Bayley, Daniel\n",
    "(n) Brown, James\n",
    "(n) Furst, Robert\n",
    "(n) Fuhs, William\n",
    "(n) Causey, Richard\n",
    "(n) Calger, Christopher\n",
    "(n) DeSpain, Timothy\n",
    "(n) Hannon, Kevin\n",
    "(n) Koenig, Mark\n",
    "(y) Forney, John\n",
    "(n) Rice, Kenneth\n",
    "(n) Rieker, Paula\n",
    "(n) Fastow, Lea\n",
    "(n) Fastow, Andrew\n",
    "(y) Delainey, David\n",
    "(n) Glisan, Ben\n",
    "(n) Richter, Jeffrey\n",
    "(n) Lawyer, Larry\n",
    "(n) Belden, Timothy\n",
    "(n) Kopper, Michael\n",
    "(n) Duncan, David\n",
    "(n) Bowen, Raymond\n",
    "(n) Colwell, Wesley\n",
    "(n) Boyle, Dan\n",
    "(n) Loehr, Christopher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(poi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(enron_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total stock belonging to  James Prentice\n",
    "enron_data['PRENTICE JAMES']['total_stock_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['PRENTICE JAMES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find num of emails Wesley Colwell sent to poi\n",
    "enron_data['COLWELL WESLEY']['from_this_person_to_poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stock options excercised by  Jeffrey K Skilling\n",
    "enron_data['SKILLING JEFFREY K']['exercised_stock_options']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who took home the most money ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['LAY KENNETH L']['total_payments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['SKILLING JEFFREY K']['total_payments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['FASTOW ANDREW S']['total_payments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many folks in this dataset have a quantified salary? What about a known email address?\n",
    "count_salary = 0\n",
    "count_email = 0\n",
    "for user in enron_data:\n",
    "    if enron_data[user]['salary'] != 'NaN':\n",
    "        #email[user] = enron_data[user]['email_address']\n",
    "        count_salary+=1\n",
    "print (count_salary)\n",
    "\n",
    "for user in enron_data:\n",
    "    if enron_data[user]['email_address'] != 'NaN':\n",
    "        count_email+=1\n",
    "print (count_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many people in the E+F dataset (as it currently exists) have “NaN” for their total payments? \n",
    "#What percentage of people in the dataset as a whole is this?\n",
    "\n",
    "count = 0\n",
    "#count_email = 0\n",
    "for user in enron_data:\n",
    "    if enron_data[user]['total_payments'] == 'NaN':\n",
    "        #email[user] = enron_data[user]['email_address']\n",
    "        count+=1\n",
    "print (count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count*100/len(enron_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ./enron/ud120-projects-master/tools/feature_format.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many POIs in the E+F dataset have “NaN” for their total payments? What percentage of POI’s as a whole is this?\n",
    "count_NaN_tp = 0\n",
    "for key in enron_data.keys():\n",
    "    if enron_data[key]['total_payments'] == 'NaN' and enron_data[key]['poi'] == True :\n",
    "        count_NaN_tp+=1\n",
    "print(count_NaN_tp)\n",
    "print(float(count_NaN_tp)/len(enron_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy on Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "# Limit to numeric data\n",
    "X = X._get_numeric_data()\n",
    "# Separate the labels\n",
    "y = X['Survived']\n",
    "# Remove labels from the inputs, and age due to missing data\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The decision tree classifier\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train,y_train)\n",
    "print(\"Decision Tree has accuracy: \",accuracy_score(clf1.predict(X_test),y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The naive Bayes classifier\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train,y_train)\n",
    "print(\"GaussianNB has accuracy: \",accuracy_score(clf2.predict(X_test),y_test))\n",
    "\n",
    "answer = { \n",
    " \"Naive Bayes Score\": 0, \n",
    " \"Decision Tree Score\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(clf1.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(clf2.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for this Decision Tree:\n",
      " [[102  37]\n",
      " [ 37  47]]\n",
      "GaussianNB confusion matrix:\n",
      " [[122  48]\n",
      " [ 17  36]]\n"
     ]
    }
   ],
   "source": [
    "# In this exercise, we'll use the Titanic dataset as before, train two classifiers and\n",
    "# look at their confusion matrices. Your job is to create a train/test split in the data\n",
    "# and report the results in the dictionary at the bottom.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the default settings for train_test_split (or test_size = 0.25 if specified).\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train,y_train)\n",
    "print(\"Confusion matrix for this Decision Tree:\\n\",confusion_matrix(clf1.predict(X_test),y_test))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train,y_train)\n",
    "print(\"GaussianNB confusion matrix:\\n\",confusion_matrix(clf2.predict(X_test),y_test))\n",
    "\n",
    "#TODO: store the confusion matrices on the test sets below\n",
    "\n",
    "confusions = {\n",
    " \"Naive Bayes\": np.matrix('99,37; 40 47'),\n",
    " \"Decision Tree\": np.matrix('122 48; 17 36')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree recall: 0.55 and precision: 0.56\n",
      "GaussianNB recall: 0.43 and precision: 0.68\n"
     ]
    }
   ],
   "source": [
    "# As with the previous exercises, let's look at the performance of a couple of classifiers\n",
    "# on the familiar Titanic dataset. Add a train/test split, then store the results in the\n",
    "# dictionary provided.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train, y_train)\n",
    "print(\"Decision Tree recall: {:.2f} and precision: {:.2f}\".format(recall(y_test,clf1.predict(X_test)),precision(y_test,clf1.predict(X_test))))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train, y_train)\n",
    "print(\"GaussianNB recall: {:.2f} and precision: {:.2f}\".format(recall(y_test,clf2.predict(X_test)),precision(y_test,clf2.predict(X_test))))\n",
    "\n",
    "results = {\n",
    "  \"Naive Bayes Recall\": 0.43,\n",
    "  \"Naive Bayes Precision\": 0.56,\n",
    "  \"Decision Tree Recall\": 0.56,\n",
    "  \"Decision Tree Precision\": 0.56\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 score: 0.56\n",
      "GaussianNB F1 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "# As usual, use a train/test split to get a reliable F1 score from two classifiers, and\n",
    "# save it the scores in the provided dictionaries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train, y_train)\n",
    "print(\"Decision Tree F1 score: {:.2f}\".format(f1_score(y_test, clf1.predict(X_test))))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train, y_train)\n",
    "print(\"GaussianNB F1 score: {:.2f}\".format(f1_score(y_test, clf2.predict(X_test))))\n",
    "\n",
    "F1_scores = {\n",
    " \"Naive Bayes\": 0.53,\n",
    " \"Decision Tree\": 0.55\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute and squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "from sklearn.datasets import load_linnerud\n",
    "\n",
    "linnerud_data = load_linnerud()\n",
    "X = linnerud_data.data\n",
    "y = linnerud_data.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg1.fit(X_train, y_train)\n",
    "print(\"Decision Tree mean absolute error: {:.2f}\".format(mae(y_test,reg1.predict(X_test))))\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(X_train, y_train)\n",
    "print(\"Linear regression mean absolute error: {:.2f}\".format(mae(y_test,reg2.predict(X_test))))\n",
    "\n",
    "reg3 = DecisionTreeRegressor()\n",
    "reg3.fit(X_train, y_train)\n",
    "print(\"Decision Tree mean absolute error: {:.2f}\".format(mse(y_test, reg3.predict(X_test))))\n",
    "\n",
    "reg4 = LinearRegression()\n",
    "reg4.fit(X_train, y_train)\n",
    "print(\"Linear regression mean absolute error: {:.2f}\".format(mse(y_test, reg4.predict(X_test))))\n",
    "\n",
    "results = {\n",
    " \"MAE Linear Regression\": 11.45,\n",
    " \"MAE Decision Tree\": 5.47,\n",
    " \"MSE Linear Regression\": 380.46,\n",
    " \"MSE Decision Tree\": 93.52\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook.nbextensions.check_nbextension('usability/collapsible_headings', user=True)\n",
    "E = notebook.nbextensions.EnableNBExtensionApp()\n",
    "E.enable_nbextension('usability/collapsible_headings/main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this exercise we'll examine a learner which has high variance, and tries to learn\n",
    "# nonexistant patterns in the data.\n",
    "# Use the learning curve function from sklearn.learning_curve to plot learning curves\n",
    "# of both training and testing error.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Set the learning curve parameters; you'll need this for learning_curves\n",
    "size = 1000\n",
    "cv = KFold(size,shuffle=True)\n",
    "score = make_scorer(explained_variance_score)\n",
    "\n",
    "# Create a series of data that forces a learner to have high variance\n",
    "X = np.round(np.reshape(np.random.normal(scale=5,size=2*size),(-1,2)),2)\n",
    "y = np.array([[np.sin(x[0]+np.sin(x[1]))] for x in X])\n",
    "\n",
    "def plot_curve():\n",
    "    reg = DecisionTreeRegressor()\n",
    "    reg.fit(X,y)\n",
    "    print(\"Regressor score: {:.4f}\".format(reg.score(X,y)))\n",
    "    training_sizes = np.linspace(0.1,1,30)\n",
    "    print(training_sizes)\n",
    "    # TODO: Use learning_curve imported above to create learning curves for both the\n",
    "    #       training data and testing data. You'll need 'size', 'cv' and 'score' from above.\n",
    "    \n",
    "    training_sizes, training_scores, testing_scores = learning_curve(reg, X, y, training_sizes, cv = cv, scoring = score)\n",
    "\n",
    "    # TODO: Plot the training curves and the testing curves\n",
    "    #       Use plt.plot twice -- one for each score. Be sure to give them labels!\n",
    "    training_scores_mean = np.mean(training_scores,axis = 1)\n",
    "    testing_scores_mean = np.mean(testing_scores,axis = 1)\n",
    "    plt.plot(training_sizes, training_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(training_sizes, testing_scores_mean, 'o-', color=\"g\", label=\"Testing score\")\n",
    "    \n",
    "    # Plot aesthetics\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.ylabel(\"Curve Score\")\n",
    "    plt.xlabel(\"Training Points\")\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor score: 1.0000\n",
      "[ 0.1         0.13103448  0.16206897  0.19310345  0.22413793  0.25517241\n",
      "  0.2862069   0.31724138  0.34827586  0.37931034  0.41034483  0.44137931\n",
      "  0.47241379  0.50344828  0.53448276  0.56551724  0.59655172  0.62758621\n",
      "  0.65862069  0.68965517  0.72068966  0.75172414  0.78275862  0.8137931\n",
      "  0.84482759  0.87586207  0.90689655  0.93793103  0.96896552  1.        ]\n"
     ]
    }
   ],
   "source": [
    "plot_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights.astype(float) \n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        # TODO: for each data point...\n",
    "        for y,lst,i in zip(train,values, range(len(self.weights))):\n",
    "            # TODO: obtain the neuron's prediction for that point\n",
    "            y_hat = self.activate(lst)\n",
    "\n",
    "            # TODO: update self.weights based on prediction accuracy, learning\n",
    "            # rate and input value\n",
    "            deltaWeight = eta*(y - y_hat)*self.weights[i]\n",
    "            self.weights[i] = self.weights[i] + deltaWeight\n",
    "        \n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n",
    "for row in dataset:\n",
    "\tprediction = predict(row, weights)\n",
    "\tprint(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#activation = (w1 * X1) + (w2 * X2) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights\n",
    "\n",
    "# Calculate weights\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    "\n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tweights = train_weights(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(row, weights)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar_all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "type(features_train)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "\n",
    "\n",
    "\n",
    "#### store your predictions in a list named pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71748878923766812"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "clf = SVC(kernel = \"linear\", C = 1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Generating random data: 20 observations of 2 features and divide into two classes.\n",
    "np.random.seed(5)\n",
    "A = np.random.randn(20,2)\n",
    "B = np.repeat([1,-1], 10)\n",
    "A[B == -1] = A[B == -1] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x8a1bf50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEPCAYAAACzwehFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWIiISFgqFiIiEpaKhYiIhPX/XoWwlsOxr1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85fae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(A[:,0], A[:,1], s=70, c=B, cmap=mpl.cm.Paired)\n",
    "plt.xlabel('A1')\n",
    "plt.ylabel('A2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44122749, -0.33087015],\n",
       "       [ 2.43077119, -0.25209213],\n",
       "       [ 0.10960984,  1.58248112],\n",
       "       [-0.9092324 , -0.59163666],\n",
       "       [ 0.18760323, -0.32986996],\n",
       "       [-1.19276461, -0.20487651],\n",
       "       [-0.35882895,  0.6034716 ],\n",
       "       [-1.66478853, -0.70017904],\n",
       "       [ 1.15139101,  1.85733101],\n",
       "       [-1.51117956,  0.64484751],\n",
       "       [ 0.01939211,  0.14314685],\n",
       "       [ 0.12812082,  0.57749207],\n",
       "       [ 1.99643983,  1.71242127],\n",
       "       [ 1.05914424,  0.63668912],\n",
       "       [ 1.00328884,  0.89406956],\n",
       "       [ 1.79305332,  0.36842837],\n",
       "       [ 0.99380509,  0.89893239],\n",
       "       [ 0.94769185,  1.24921766],\n",
       "       [ 1.19766009,  2.33484857],\n",
       "       [ 0.91312439,  2.56153229]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "#from udacityplots import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib \n",
    "matplotlib.use('agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ioff()\n",
    "\n",
    "def prettyPicture(clf, X_test, y_test):\n",
    "    x_min = 0.0; x_max = 1.0\n",
    "    y_min = 0.0; y_max = 1.0\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h = .01  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)\n",
    "\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"fast\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"slow\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "\n",
    "    plt.savefig(\"test.png\")\n",
    "    \n",
    "import base64\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "def output_image(name, format, bytes):\n",
    "    image_start = \"BEGIN_IMAGE_f9825uweof8jw9fj4r8\"\n",
    "    image_end = \"END_IMAGE_0238jfw08fjsiufhw8frs\"\n",
    "    data = {}\n",
    "    data['name'] = name\n",
    "    data['format'] = format\n",
    "    data['bytes'] = base64.encodestring(bytes)\n",
    "    print(image_start+json.dumps(data)+image_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep_terrain_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "\n",
    "def makeTerrainData(n_points=1000):\n",
    "###############################################################################\n",
    "### make the toy dataset\n",
    "    random.seed(42)\n",
    "    grade = [random.random() for ii in range(0,n_points)]\n",
    "    bumpy = [random.random() for ii in range(0,n_points)]\n",
    "    error = [random.random() for ii in range(0,n_points)]\n",
    "    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]\n",
    "    for ii in range(0, len(y)):\n",
    "        if grade[ii]>0.8 or bumpy[ii]>0.8:\n",
    "            y[ii] = 1.0\n",
    "\n",
    "### split into train/test sets\n",
    "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
    "    split = int(0.75*n_points)\n",
    "    X_train = X[0:split]\n",
    "    X_test  = X[split:]\n",
    "    y_train = y[0:split]\n",
    "    y_test  = y[split:]\n",
    "\n",
    "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "\n",
    "#    training_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "#            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    test_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "#    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifyNB.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'prep_terrain_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e1d566075cdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprep_terrain_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmakeTerrainData\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclass_vis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprettyPicture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#from ClassifyNB import classify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'prep_terrain_data'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from class_vis import prettyPicture, output_image\n",
    "#from ClassifyNB import classify\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "def classify(features_train, labels_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    return clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# StudentsMain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'prep_terrain_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3b9c59c73cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprep_terrain_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmakeTerrainData\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclass_vis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprettyPicture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mClassifyNB\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'prep_terrain_data'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from class_vis import prettyPicture, output_image\n",
    "from ClassifyNB import classify\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist_id": "8e2ebee37c8e7aaa10fd48acef1e5e52",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
